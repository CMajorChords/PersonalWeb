
<h2 style='pointer-events: none;'>pytorch基础</h2>
<h3 style='pointer-events: none;'>1.torch张量常用操作</h3>

```
import torch
```
生成空张量
```
x = torch.empty(1, 1, 1, 1) # 生成一个四维的tensor
y = torch.empty(1, 1) # 生成一个二维的tensor
```
生成随机张量
```
x = torch.rand(1, 1, 1, 1) # 生成一个四维的tensor
y = torch.rand(1, 1) # 生成一个二维的tensor
```
torch元素级别的操作
```
x = torch.rand(1, 1,) # 生成一个四维的tensor
y = torch.rand(1, 1,) # 生成一个二维的tensor
z_add = torch.add(x, y) # 加法
z_sub = torch.sub(x, y) # 减法
z_mul = torch.mul(x, y) # 乘法
z_div = torch.div(x, y) # 除法
```
torch矩阵操作
```
x = torch.rand(1, 1, 1, 1) # 生成一个四维的tensor
y = torch.rand(1, 1, 1, 1) # 生成一个四维的tensor
z = torch.mm(x, y) # 矩阵乘法
z = torch.t(x) # 转置
```
torch索引、切片与形状
```
x = torch.rand(5, 5, 5, 5) # 生成一个四维的tensor
z = x[0, 0, 0, 0] # 获取第一个元素
z = x[0, 0, 0, :] # 获取第四维的所有元素，即一个一维的tensor
z = x[0, 0, :, :] # 获取第三维的所有元素，即一个二维的tensor
z = x[0, :, :, :] # 获取第二维的所有元素， 即一个三维的tensor
z = x[:, :, :, :] # 获取所有元素
z = x[0:2, 0:2, 0:2, 0:2] # 获得一个2*2*2*2的tensor
z = x.view(625) # 将一个四维的tensor转换成一个一维的tensor
z = x.view(25, 25) # 将一个四维的tensor转换成一个二维的tensor
x_shape = x.shape # 获取tensor的形状
x_shape = x.size() # 获取tensor的形状，与shape相同
```
torch张量拼接
```
x = torch.rand(5, 5,) # 生成一个二维的tensor
y = torch.rand(5, 5,) # 生成一个二维的tensor
z = torch.cat((x, y), 0) # 按照行拼接
z = torch.cat((x, y), 1) # 按照列拼接
```
torch张量的广播机制
```
x = torch.rand(5, 5,) # 生成一个二维的tensor
y = torch.rand(5, 1,) # 生成一个二维的tensor
z = x + y # 二维张量的广播机制
```
torch张量的自动求导
```
x = torch.rand(2, 2, requires_grad=True) # requires_grad=True表示需要求导
y = torch.rand(2, 2, requires_grad=True)
z = x + y + 2
z = z.mean() # 求均值，均值的梯度应该是1/n，n是元素个数
z.backward() # 反向传播 dz/dx dz/dy
x_grad = x.grad # x的梯度应该是1/4
y_grad = y.grad # y的梯度应该是1/4
```
线性回归的简洁实现
```
import torch
# f = w * x
X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)
w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)

# 前向传播
def forward(x):
    return w * x

# 损失函数为均方误差
def loss(y, y_predicted):
    return ((y_predicted - y)**2).mean()

# 梯度下降
learning_rate = 0.01
n_iters = 100

# 模型训练
for epoch in range(n_iters):
    # 前向传播
    y_pred = forward(X)
    # 计算损失
    l = loss(Y, y_pred)
    # 反向传播
    l.backward() # dl/dw
    # 更新权重
    with torch.no_grad():
        w -= learning_rate * w.grad
    # 梯度清零
    w.grad.zero_()
    if epoch % 1 == 0:
        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')
```
<h3 style='pointer-events: none;'>2.pytorch中的DataLoader和Dataset</h3>

在训练时，我们需要将数据分批次的输入到模型中，这时候就需要用到DataLoader和Dataset。Dataset是一个抽象类，我们需要继承这个类并实现__len__和__getitem__两个方法。__len__方法返回数据集的大小，__getitem__方法返回数据集中的一个样本。DataLoader是一个迭代器，它可以将数据集分批次的输出。

```
# training loop
for epoch in range(epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        ...
```
<h4 style='pointer-events: none;'>Dataset & DataLoader</h4>

可以通过继承Dataset类来实现自己的数据集。这里定义一个专门用于bp神经网络的水文数据集
```
import torch
import pandas as pd

class HydroDatasetForBP(torch.utils.data.Dataset):

    def __init__(self, data, target):
        data = pd.read_excel("Data/澴水流域数据.xlsx")
        # 特征从第三列开始 | features start from the third column
        self.x = torch.tensor(data.iloc[:, 2:].values, dtype=torch.float32)
        # 标签为第二列 | label is the second column
        self.y = torch.tensor(data.iloc[:, 1].values, dtype=torch.float32)
        # 样本数量 | number of samples
        self.n_samples = data.shape[0]
        
    def __getitem__(self, index):
        return self.x[index], self.y[index]
    
    def __len__(self):
        return self.n_samples

dataset = HydroDatasetForBP()
dataloder = torch.utils.data.DataLoader(
                dataset=dataset, 
                batch_size=64, 
                shuffle=True,
                num_workers=2, # 多线程读取数据 | read data in multiple threads
            )
```
<h4 style='pointer-events: none;'>训练</h4>

```
num_epochs = 100
total_samples = len(dataset)
# 每个epoch需要迭代的次数 | number of iterations per epoch
n_iterations = math.ceil(total_samples/64)
for epoch in range(num_epochs):
    for i, (inputs, labels) in enumerate(dataloader):
        # forward backward, update
        ...
        if (i+1) % 5 == 0:
            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')
```
<h4 style='pointer-events: none;'>Dataset Transforms</h4>

水文上主要是对径流过程或是各类水文气象数据的处理，所以主要是对tensor的操作，图像操作也能学，随你  
```
import torch
import pandas as pd
import torchvision
import torchtext

# 从上面复制过来的额 | copied from above
class HydroDatasetForBP(torch.utils.data.Dataset):

    def __init__(self, data, target, transform=None):
        data = pd.read_excel("Data/澴水流域数据.xlsx")
        # 特征从第三列开始  |  features start from the third column
        self.x = data.iloc[:, 2:].values
        # 标签为第二列 | label is the second column
        self.y = data.iloc[:, 1].values
        # 样本数量 | number of samples
        self.n_samples = data.shape[0]
        self.transform = transform
    
    def __getitem__(self, index):
        sample = self.x[index], self.y[index]
        if self.transform:
            sample = self.transform(sample)
        return sample
    
    def __len__(self):
        return self.n_samples
# 写两个转换 | write two transforms
class ToTensor:
    """
    对数据进行转换 | transform data
    numpy array -> tensor
    """
    def __call__(self, data):
        inputs, targets = data
        return torch.from_numpy(inputs), torch.from_numpy(targets)

class Scale:
    """
    对数据进行缩放 | scale data
    """
    def __init__(self, data_max)
        self.data_max = data_max
    
    def __call__(self, data):
        inputs, targets = data
        return inputs/self.data_max, targets/self.data_max

# 把两个转换组合起来 | combine two transforms
composed = torchvision.transforms.Compose([ToTensor(), Scale(1000)])
# 创建数据集 | create dataset
dataset = HydroDatasetForBP(transform=composed)
```
<h3 style='pointer-events: none;'>3.Pytorch模型设计工作流</h3>

```
import torch
import torch.nn as nn
```
<h4 style='pointer-events: none;'>问题定义</h4>

模型的输入、输出应该是什么，大小有多大，是一个什么类型的问题  
```
X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)
Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)
X_test = torch.tensor([5], dtype=torch.float32)
n_samples, n_features = X.shape # sample代表样本数，feature代表特征数 sample represents the number of samples, feature represents the number of features
input_size = n_features # 输入大小 input size
output_size = n_features # 输出大小 output size
```
<h4 style='pointer-events: none;'>模型设计</h4>

根据问题定义模型的前向传播过程应该是怎么样的  
```
class LinearRegression(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegression, self).__init__()
        self.lin = nn.Linear(input_dim, output_dim)
    def forward(self, x):
        return self.lin(x)
model = LinearRegression(input_size, output_size)
```
定义损失函数和优化器
```  
loss = nn.MSELoss()
learning_rate = 0.01
optimizer = torch.optim.SGD([model.parameters()], lr=learning_rate)
```
<h4 style='pointer-events: none;'>训练模型</h4>

```
n_iters = 1000
for epoch in range(n_iters):
    y_pred = model(X)
    l = loss(Y, y_pred)
    l.backward()
    optimizer.step()
    optimizer.zero_grad()
    if epoch % 10 == 0:
        [w, b] = model.parameters()
        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}') 
```
<h4 style='pointer-events: none;'>测试模型</h4>

```
print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')
```
<h4 style='pointer-events: none;'>例子：线性回归</h4>

```
import torch
import torch.nn as nn
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
```
准备数据
```
X_numpy, Y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)
X = torch.from_numpy(X_numpy.astype(np.float32))
Y = torch.from_numpy(Y_numpy.astype(np.float32))
# 把Y的维度从(100,)变成(100,1)|change the dimension of Y from (100,) to (100,1)
Y = Y.view(Y.shape[0], 1)
n_samples, n_features = X.shape
```
模型设计
```
input_size = n_features
output_size = 1
class LinearRegression(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegression, self).__init__()
        self.lin = nn.Linear(input_dim, output_dim)
    def forward(self, x):
        return self.lin(x)
```
损失函数和优化器
```
learning_rate = 0.01
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
```
训练模型
```
num_epochs = 100
for epoch in range(num_epochs):
    # forward pass and loss
    y_predicted = model(X)
    loss = criterion(y_predicted, Y)
    # backward pass
    loss.backward()
    # update
    optimizer.step()
    optimizer.zero_grad()
    if (epoch+1) % 10 == 0:
        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')
```
画图
```
# 首先把X和Y转换成numpy数组 | First convert X and Y to numpy arrays
predicted = model(X).detach().numpy()
plt.plot(X_numpy, Y_numpy, 'ro')
plt.plot(X_numpy, predicted, 'b')
plt.show()
```
<h4 style='pointer-events: none;'>例子：逻辑回归</h4>

```
import torch
import torch.nn as nn
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
```
准备数据
```
# 用sklearn的数据集 | Use sklearn's dataset
bc = datasets.load_breast_cancer()
# 二分类问题，一共有569个样本，每个样本有30个特征，y是0或1
# Binary classification problem, 569 samples in total, 30 features per sample, y is 0 or 1
X, Y = bc.data, bc.target
n_samples, n_features = X.shape
# 划分训练集和测试集 | Divide the training set and test set
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)
# 放缩到[-1,1] | Scale to [-1,1]
scaler = StandardScaler()
# 把训练集特征都放缩到[-1,1] | Scale the features of the training set and test set to [-1,1]
X_train = scaler.fit_transform(X_train)
# 把测试集特征都放缩到[-1,1] | Scale the features of the test set to [-1,1]
X_test = scaler.transform(X_test) # 注意这里不是fit_transform，因为测试集的放缩要用训练集的放缩器
# 把numpy数组转换成张量 | Convert numpy arrays to tensors
X_test = torch.from_numpy(X_test.astype(np.float32))
Y_train = torch.from_numpy(Y_train.astype(np.float32))
Y_test = Y_test.view(Y_test.shape[0], 1)
Y_train = Y_train.view(Y_train.shape[0], 1)
```
模型设计
```
class LogisticRegression(nn.Module):
    def __init__(self, n_input_features):
        super(LogisticRegression, self).__init__()
        # 1表示输出的维度是1|1 means the output dimension is 1
        self.lin = nn.Linear(n_input_features, 1)
    def forward(self, x):
        y_predicted = torch.sigmoid(self.lin(x))
        return y_predicted

model = LogisticRegression(n_features)
```
损失函数和优化器
```
learning_rate = 0.01
criterion = nn.BCELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
```
训练模型
```
num_epochs = 100
for epoch in range(num_epochs):
    # forward pass and loss
    y_predicted = model(X_train)
    loss = criterion(y_predicted, Y_train)
    # backward pass
    loss.backward()
    # update
    optimizer.step()
    optimizer.zero_grad()
    if (epoch+1) % 10 == 0:
        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')
```
模型评估
```
with torch.no_grad():
    y_predicted = model(X_test)
    y_predicted_cls = y_predicted.round()
    acc = y_predicted_cls.eq(Y_test).sum() / float(Y_test.shape[0])
    print(f'accuracy = {acc:.4f}')
```
