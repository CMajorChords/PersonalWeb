<h2 style='pointer-events: none;'>基建：CAMELS、时序数据集处理和检查</h2>

utils/data/check.py
```
# 创造检查数据的类和函数
from typing import Union, Sequence
from numpy import ndarray, isnan
from concurrent.futures import ProcessPoolExecutor
from pandas import Series, DataFrame, concat


def check_nan_pandas(data: Union[DataFrame, Series]) -> Series:
    """
    检查数据中是否存在缺失值。

    :param data: 待检查的数据，
    :return: 检查结果, 返回存在缺失值列的缺失值数量
    """
    if isinstance(data, Series):
        data = data.to_frame()
    # 检查每一列是否存在缺失值并计算缺失值的数量
    nan_count = data.isnull().sum()
    nan_count.name = "nan_count"
    return nan_count[nan_count > 0]


def check_nan_numpy(data: Union[Sequence[ndarray], ndarray]) -> bool:
    """
    检查数据中是否存在缺失值。

    :param data: 待检查的数据，可以是多个ndarray组成的list或者tuple
    :return: 检查结果, 返回存在缺失值列的缺失值数量
    """
    return any((isnan(sub_data).any() for sub_data in data)) if isinstance(data, Sequence) else isnan(data).any()


def append_na_sequences(na_sequences: DataFrame,
                        na_sequence_start,
                        na_sequence_end,
                        na_sequence_length: int) -> DataFrame:
    """
    将连续缺失值的信息存入DataFrame。
    :param na_sequences: 存储连续缺失值的DataFrame
    :param na_sequence_start: 连续缺失值的起始时间
    :param na_sequence_end: 连续缺失值的结束时间
    :param na_sequence_length: 连续缺失值的长度
    :return: 存入连续缺失值信息后的DataFrame
    """
    # 将该段连续缺失值的信息存入DataFrame
    return concat([na_sequences, DataFrame({"start_time": [na_sequence_start],
                                            "end_time": [na_sequence_end],
                                            "length": [na_sequence_length]})],
                  axis=0,
                  ignore_index=True)


def check_consecutive_na_in_series(data: Series, threshold: int) -> DataFrame:
    """
    检查Series数据中是否存在连续缺失值。
    :param data: 要检查的数据
    :param threshold: 连续缺失值的阈值
    :return: 返回一个DataFrame，包含连续缺失值的起始时间、结束时间和长度
    """
    # 找到当前列的na值
    data_is_na = data.isna()
    # 得到当前列的长度
    time_length = len(data_is_na)
    # 初始化
    prev_is_na = False  # 用于记录前一个值是否为na
    na_sequences = DataFrame()  # 用于存储连续缺失值的信息
    na_sequence_start = None  # 用于记录连续缺失值的起始时间
    na_sequence_length = 0  # 用于记录连续缺失值的长度
    # 寻找连续缺失值
    for i, (time_index, is_na) in enumerate(data_is_na.items()):
        if is_na and not prev_is_na:  # 当前值为na，前一个值不为na，说明是连续缺失值的起始位置
            na_sequence_length = 1
            na_sequence_start = time_index
            # 判断当前值是否是最后一个值
            if i == time_length - 1 and threshold == 1:
                na_sequences = append_na_sequences(na_sequences, na_sequence_start, time_index, na_sequence_length)
        elif is_na and prev_is_na:  # 当前值为na，前一个值也为na，说明是连续缺失值的中间位置
            na_sequence_length += 1
            # 判断当前值是否是最后一个值
            if i == time_length - 1 and na_sequence_length >= threshold:
                na_sequences = append_na_sequences(na_sequences, na_sequence_start, time_index, na_sequence_length)
        elif not is_na and prev_is_na:  # 当前值不为na，前一个值为na，说明是连续缺失值的结束位置
            na_sequence_end = data_is_na.index[i - 1]
            # 将该段连续缺失值的信息存入DataFrame
            if na_sequence_length >= threshold:  # 连续缺失值的长度大于等于阈值
                na_sequences = append_na_sequences(na_sequences, na_sequence_start, na_sequence_end, na_sequence_length)
            # 重置
            na_sequence_start = None
            na_sequence_length = 0
        prev_is_na = is_na
    return na_sequences


def check_consecutive_na_in_dataframe(data: DataFrame, threshold: int) -> DataFrame:
    """
    检查数据中是否存在连续缺失值。
    :param data: 要检查的数据
    :param threshold: 连续缺失值的阈值
    :return: 返回一个DataFrame，其中index是一个multiindex，level为(列名, 起始时间)，columns为[结束时间，连续缺失值长度]
    """
    with ProcessPoolExecutor() as executor:
        result = concat(
            executor.map(check_consecutive_na_in_series,
                         [data[column] for column in data.columns],
                         [threshold] * len(data.columns)),
            keys=data.columns,
            axis=0,
            names=["column", "na_sequence_index"]
        )
    return result


def check_consecutive_nas(data: Union[DataFrame, Series], threshold: int = 10) -> DataFrame:
    """
    检查数据中是否存在连续缺失值。

    :param data: 要检查的数据
    :param threshold: 连续缺失值的阈值
    :return: 返回一个DataFrame，其中index是一个multiindex，level为(列名, 起始时间)，columns为[结束时间，连续缺失值长度]
    """
    if isinstance(data, DataFrame):
        check_result = check_consecutive_na_in_dataframe(data=data, threshold=threshold)
    else:
        check_result = check_consecutive_na_in_series(data=data, threshold=threshold)
    return check_result.astype({"start_time": "datetime64[ns]", "end_time": "datetime64[ns]", "length": "int"})
```
utils/data/dataset.py
```
# 构建用于时间序列的高性能Dataset和DataLoader
from torch.utils.data import Dataset
from torch import Tensor
from concurrent.futures import ProcessPoolExecutor
from pandas import DataFrame, Series
from utils.data.check import check_nan_numpy
from typing import Union, Optional, List, Tuple
import numpy as np
from numpy import ndarray, expand_dims, vstack


def map_target_and_features_lookback(data: DataFrame,
                                     target: str,
                                     features_lookback: Optional[List[str]],
                                     lookback: int,
                                     horizon: int,
                                     ) -> Tuple[ndarray, ndarray]:
    """
    将目标时间序列映射为输入和输出。用于所有特征的未来值都是未知的情况。

    :param data: 时间序列数据，可以是DataFrame或者Series，列索引不能为multiindex
    :param target: 目标时间序列的列名
    :param lookback: 过去时间步数
    :param horizon: 未来时间步数
    :param features_lookback: 只将lookback输入到网络的特征。注意绝大多数自回归都需要将target_col作为features_lookback的一部分
    :return: 目标时间序列的输入和输出
    """
    # 从DataFrame中提取目标时间序列和特征时间序列
    target = data[target].values
    features_lookback = data[features_lookback].values
    # 初始化输入和输出
    input_data = np.empty((0, lookback, features_lookback.shape[1]))
    output_data = np.empty((0, horizon))
    length = target.shape[0] - lookback - horizon + 1
    # 循环提取每个输入-输出对
    for i in range(length):
        i_input = features_lookback[i:i + lookback]
        i_output = target[i + lookback:i + lookback + horizon]
        if check_nan_numpy([i_input, i_output]):
            continue
        else:
            i_input = expand_dims(i_input, axis=0)
            i_output = expand_dims(i_output, axis=0)
            input_data = vstack((input_data, i_input))
            output_data = vstack((output_data, i_output))
    return input_data, output_data


def map_target_and_features_bidirectional(data: DataFrame,
                                          target: str,
                                          features_bidirectional: Optional[List[str]],
                                          lookback: int,
                                          horizon: int,
                                          ) -> Tuple[ndarray, ndarray]:
    """
    将目标时间序列映射为输入和输出。用于全部特征的未来值都是已知的情况。

    :param data: 时间序列数据，可以是DataFrame或者Series，列索引不能为multiindex
    :param target: 目标时间序列的列名
    :param lookback: 过去时间步数
    :param horizon: 未来时间步数
    :param features_bidirectional: 将lookback和horizon全部输入到网络的特征。例如气象数据可以提前预测再输入到降水径流模型，时间列也是可以提前知道的
    :return: 目标时间序列的输入和输出
    """
    # 从DataFrame中提取目标时间序列和特征时间序列
    target = data[target].values
    features_bidirectional = data[features_bidirectional].values
    # 初始化输入和输出
    input_data = np.empty((0, lookback + horizon, features_bidirectional.shape[1]))
    output_data = np.empty((0, horizon))
    length = target.shape[0] - lookback - horizon + 1
    # 循环提取每个输入-输出对
    for i in range(length):
        i_input = features_bidirectional[i:i + lookback + horizon]
        i_output = target[i + lookback:i + lookback + horizon]
        if check_nan_numpy([i_input, i_output]):
            continue
        else:
            i_input = expand_dims(i_input, axis=0)
            i_output = expand_dims(i_output, axis=0)
            input_data = vstack((input_data, i_input))
            output_data = vstack((output_data, i_output))
    return input_data, output_data


def map_target_and_all_type_features(data: DataFrame,
                                     target: str,
                                     features_lookback: Optional[List[str]],
                                     features_bidirectional: Optional[List[str]],
                                     lookback: int,
                                     horizon: int,
                                     ) -> Tuple[ndarray, ndarray, ndarray]:
    """
    将目标时间序列映射为输入和输出。用于部分特征的未来值是未知的，部分特征的未来值是已知的情况。

    :param data: 时间序列数据，可以是DataFrame或者Series，列索引不能为multiindex
    :param target: 目标时间序列的列名
    :param lookback: 过去时间步数
    :param horizon: 未来时间步数
    :param features_lookback: 只将lookback输入到网络的特征。注意绝大多数自回归都需要将target_col作为features_lookback的一部分
    :param features_bidirectional: 将lookback和horizon全部输入到网络的特征。例如气象数据可以提前预测再输入到降水径流模型，时间列也是可以提前知道的
    :return: 根据features_lookback和features_bidirectional的不同，返回三个ndarray或者两个ndarray
    """
    # 从DataFrame中提取目标时间序列和特征时间序列
    target = data[target].values
    features_lookback = data[features_lookback].values
    features_bidirectional = data[features_bidirectional].values
    # 初始化输入和输出
    input_lookback = np.empty((0, lookback, features_lookback.shape[1]))
    input_bidirectional = np.empty((0, lookback + horizon, features_bidirectional.shape[1]))
    output_data = np.empty((0, horizon))
    length = target.shape[0] - lookback - horizon + 1
    # 循环提取每个输入-输出对
    for i in range(length):
        i_input_lookback = features_lookback[i:i + lookback]
        i_input_bidirectional = features_bidirectional[i:i + lookback + horizon]
        i_output = target[i + lookback:i + lookback + horizon]
        if check_nan_numpy([i_input_lookback, i_input_bidirectional, i_output]):
            continue
        else:
            i_input_lookback = expand_dims(i_input_lookback, axis=0)
            i_input_bidirectional = expand_dims(i_input_bidirectional, axis=0)
            i_output = expand_dims(i_output, axis=0)
            input_lookback = vstack((input_lookback, i_input_lookback))
            input_bidirectional = vstack((input_bidirectional, i_input_bidirectional))
            output_data = vstack((output_data, i_output))
    return input_lookback, input_bidirectional, output_data


def map_data_for_single_basin(data: Union[DataFrame, Series],
                              target: str,
                              lookback: int,
                              horizon: int,
                              features_lookback: Optional[List[str]] = None,
                              features_bidirectional: Optional[List[str]] = None,
                              ) -> Union[Tuple[ndarray, ndarray, ndarray], Tuple[ndarray, ndarray]]:
    """
    将时间序列分别处理为目标时间序列和特征时间序列，用于构建Dataset。

    :param data: 时间序列数据，可以是DataFrame或者Series，列索引不能为multiindex
    :param target: 目标时间序列的列名
    :param lookback: 过去时间步数
    :param horizon: 未来时间步数
    :param features_lookback: 只将lookback输入到网络的特征。注意绝大多数自回归都需要将target_col作为features_lookback的一部分
    :param features_bidirectional: 将lookback和horizon全部输入到网络的特征。例如气象数据可以提前预测再输入到降水径流模型，时间列也是可以提前知道的
    :return: 根据features_lookback和features_bidirectional的不同，返回三个ndarray或者两个ndarray
    """
    match (features_lookback is None, features_bidirectional is None):
        case (True, True):
            raise ValueError("过去特征和双向特征不能同时为空")
        case (False, False):
            if set(features_lookback) & set(features_bidirectional):
                raise ValueError("过去特征和双向特征不能有重复")
            return map_target_and_all_type_features(data=data,
                                                    target=target,
                                                    features_lookback=features_lookback,
                                                    features_bidirectional=features_bidirectional,
                                                    lookback=lookback,
                                                    horizon=horizon)
        case (False, True):
            return map_target_and_features_lookback(data=data,
                                                    target=target,
                                                    features_lookback=features_lookback,
                                                    lookback=lookback,
                                                    horizon=horizon)
        case (True, False):
            return map_target_and_features_bidirectional(data=data,
                                                         target=target,
                                                         features_bidirectional=features_bidirectional,
                                                         lookback=lookback,
                                                         horizon=horizon)


def map_data_for_multiple_basins(data: DataFrame,
                                 target: str,
                                 lookback: int,
                                 horizon: int,
                                 features_lookback: Optional[List[str]] = None,
                                 features_bidirectional: Optional[List[str]] = None,
                                 ) -> Union[Tuple[ndarray, ndarray, ndarray], Tuple[ndarray, ndarray]]:
    """
    处理多个流域的时间序列数据，用于构建Dataset。

    :param data: 多个流域的时间序列数据，列索引应当为multiindex(流域id, 特征名)
    :param target: 目标时间序列的列名
    :param lookback: 过去时间步数
    :param horizon: 未来时间步数
    :param features_lookback: 只将lookback输入到网络的特征。注意绝大多数自回归都需要将target_col作为features_lookback的一部分
    :param features_bidirectional: 将lookback和horizon全部输入到网络的特征。例如气象数据可以提前预测再输入到降水径流模型，时间列也是可以提前知道的
    :return: 根据features_lookback和features_bidirectional的不同，返回三个ndarray或者两个ndarray
    """
    match (features_lookback is None, features_bidirectional is None):
        case (True, True):
            raise ValueError("过去特征和双向特征不能同时为空")
        case (False, False):
            if set(features_lookback) & set(features_bidirectional):
                raise ValueError("过去特征和双向特征不能有重复")
            # 为多进程提供参数
            params = ((group.T.droplevel(0, axis=1),  # 按照列索引的第一个level分组
                       target,
                       features_lookback,
                       features_bidirectional,
                       lookback,
                       horizon,)
                      for _, group in data.T.groupby(level=0))
            params = zip(*params)  # zip解包后的数据中每个tuple中是多个流域的一个参数
            # 多进程处理数据
            with ProcessPoolExecutor() as executor:
                input_lookback, input_bidirectional, output_data = zip(
                    *executor.map(map_target_and_all_type_features, *params))
            return (vstack(input_lookback),
                    vstack(input_bidirectional),
                    vstack(output_data))
        case (False, True):
            # 为多进程提供参数
            params = ((group.T.droplevel(0, axis=1),  # 按照列索引的第一个level分组
                       target,
                       features_lookback,
                       lookback,
                       horizon,)
                      for _, group in data.T.groupby(level=0))  # 每个tuple中是一个流域的多个参数
            params = zip(*params)  # zip解包后的数据中每个tuple中是多个流域的一个参数
            # 多进程处理数据
            with ProcessPoolExecutor() as executor:
                input_data, output_data = zip(*executor.map(map_target_and_features_lookback, *params))
            return vstack(input_data), vstack(output_data)
        case (True, False):
            # 为多进程提供参数
            params = ((group.T.droplevel(0, axis=1),  # 按照列索引的第一个level分组
                       target,
                       features_bidirectional,
                       lookback,
                       horizon,)
                      for _, group in data.T.groupby(level=0))  # 每个tuple中是一个流域的多个参数
            params = zip(*params)  # zip解包后的数据中每个tuple中是多个流域的一个参数
            # 多进程处理数据
            with ProcessPoolExecutor() as executor:
                input_data, output_data = zip(*executor.map(map_target_and_features_bidirectional, *params))
            return vstack(input_data), vstack(output_data)
```
utils/data/camels/camels.py
```
# 创建用于处理camels数据集的函数、类
import pandas as pd
from pandas import DataFrame
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from typing import List, Tuple, Union, Sequence, Optional
import utils.data.camels.camels_params as params
from os import cpu_count


def get_gauge_id(root_path: str = params.camels_root_path,
                 get_huc_02: bool = False,
                 ignore_gauge_id: Optional[List[str]] = None,
                 n: Optional[int] = None,
                 ) -> Union[
    str,
    List[str],
    Tuple[str, str],
    Tuple[List[str], List[str]]
]:
    """
    获取指定流域ID。如果没有指定流域ID，则返回所有流域ID

    :param root_path: CAMELS数据集根目录
    :param get_huc_02: 是否获取HUC02编码
    :param n: 流域ID数量，默认为选择所有流域ID
    :param ignore_gauge_id: 忽略的流域ID列表
    :return: n个随机流域ID和对应的HUC02编码（可选）
    """
    camels_name = load_single_type_attributes("name", root_path)
    # 删除忽略的流域ID
    if ignore_gauge_id is not None:
        camels_name.drop(index=ignore_gauge_id, inplace=True)
    # 如果n为None，则返回所有流域ID, 否则返回n个随机流域ID
    data = camels_name if n is None else camels_name.sample(n)
    # 如果get_huc_02为True，则返回流域ID和对应的HUC02编码，否则只返回流域ID
    gauge_id_list = data.index.tolist()
    if get_huc_02:
        huc_02_list = data["huc_02"].tolist()
        return (gauge_id_list[0], huc_02_list[0]) if n == 1 else (gauge_id_list, huc_02_list)
    else:
        return gauge_id_list[0] if n == 1 else gauge_id_list


def load_single_type_attributes(attr: str,
                                root_path: str = params.camels_root_path,
                                ) -> DataFrame:
    """
    加载CAMELS数据集中的单个属性种类的属性数据

    :param root_path: CAMELS数据集根目录
    :param attr: 属性种类名称
    :return: 单个属性种类的属性数据
    """
    path = root_path + f"/camels_{attr}.txt"
    dtype = {"gauge_id": str, "huc_02": str, } if attr == "name" else {"gauge_id": str, }
    return pd.read_csv(path,
                       index_col="gauge_id",
                       dtype=dtype,
                       sep=";",
                       )


def load_all_attributes(attr_type_name_list: Optional[List[str]] = None,
                        root_path: str = params.camels_root_path,
                        use_attr_name: Optional[List[str]] = None,
                        ) -> DataFrame:
    """
    加载CAMELS数据集中所有的流域属性数据

    :param attr_type_name_list: 属性种类名称列表
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param use_attr_name: 使用的属性名称列表
    :return: 加载指定属性种类的所有属性数据
    """
    # 首先加载指定属性种类的属性数据
    if attr_type_name_list is None:
        attr_type_name_list = params.attributes_type_name
    with ThreadPoolExecutor(max_workers=2) as executor:
        attributes = pd.concat(
            executor.map(load_single_type_attributes,
                         attr_type_name_list, [root_path] * len(attr_type_name_list)
                         ),
            axis=1)
    # 删除忽略的属性
    if use_attr_name is None:
        use_attr_name = params.use_attribute_name
    for attribute in use_attr_name:  # 检查忽略的属性是否在属性数据中
        if attribute not in attributes.columns:
            raise ValueError(f"属性{attribute}不在属性数据中")
    attributes = attributes[use_attr_name]
    return attributes


def split_list(lst: Sequence, n: int) -> List[List]:
    k, m = divmod(len(lst), n)
    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]


def load_single_basin_forcing(gauge_id: str,
                              huc_02: Optional[str] = None,
                              root_path: str = params.camels_root_path,
                              source: str = params.precipitation_source,
                              ignore_columns: Optional[List[str]] = None,
                              add_datetime: bool = True,
                              ) -> DataFrame:
    """
    加载单个流域的降水数据

    :param gauge_id: 流域ID
    :param huc_02: 流域的HUC02编码
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param source: 降水数据来源, 默认为"daymet", 可选值为"daymet"、“maurer"和”nldas"
    :param ignore_columns: 忽略的列名列表
    :param add_datetime: 是否添加datetime列，若添加，将删去Year，Mnth和Day三列，并添加datetime列为索引
    :return: 单个流域的降水数据
    """
    # 如果没有指定HUC02编码，则从流域名称数据中获取
    if huc_02 is None:
        huc_02 = load_single_type_attributes("name", root_path).loc[gauge_id, "huc_02"]
    # 根据降水数据来源获得数据路径
    path = root_path + "/basin_timeseries_v1p2_metForcing_obsFlow/basin_dataset_public_v1p2/basin_mean_forcing/" + source
    if source == "daymet":
        path = path + f"/{huc_02}/{gauge_id}_lump_cida_forcing_leap.txt"
        drop_columns = ["Hr", "dayl(s)", "swe(mm)"]
    elif source == "maurer":
        path = path + f"/{huc_02}/{gauge_id}_lump_maurer_forcing_leap.txt"
        drop_columns = ["Hr", "dayl(s)", "swe(mm)"]
    elif source == "nldas":
        path = path + f"/{huc_02}/{gauge_id}_lump_nldas_forcing_leap.txt"
    else:
        raise ValueError(f"不支持的数据来源{source}")
    # 读取并修改数据
    data = pd.read_csv(path,
                       skiprows=3,
                       sep="\s+",
                       )
    if add_datetime:  # 根据年月日形成datetime列
        data["datetime"] = pd.to_datetime(data[["Year", "Mnth", "Day"]].astype(str).agg("-".join, axis=1))
        data.set_index("datetime", inplace=True)
        data.drop(columns=["Year", "Mnth", "Day"], inplace=True)
    if ignore_columns is None:
        ignore_columns = params.ignore_precip_cols_daymet if source == "daymet" else params.ignore_precip_cols_maurer_and_nldas
    data.drop(columns=ignore_columns, inplace=True)
    return data


def load_basins_forcing_with_threads(gauge_id_list: List[str],
                                     huc_02_list: List[str],
                                     root_path: str,
                                     source: str,
                                     ignore_columns: Optional[List[str]],
                                     ) -> DataFrame:
    """
    多线程加载CAMELS数据集中部分流域的降水数据
    :param gauge_id_list: 流域ID列表
    :param huc_02_list: 流域的HUC02编码列表
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param source: 降水数据来源, 默认为"daymet", 可选值为"daymet"、“maurer"和”nldas"
    :param ignore_columns: 忽略的列名列表
    :return:
    """
    with ThreadPoolExecutor() as executor:  # 使用多线程加载所有流域的降水数据
        return pd.concat(
            executor.map(load_single_basin_forcing,
                         gauge_id_list, huc_02_list,
                         [root_path] * len(gauge_id_list),
                         [source] * len(gauge_id_list),
                         [ignore_columns] * len(gauge_id_list),
                         ),
            keys=gauge_id_list,
            names=["gauge_id", "timeseries_type"],
            axis=1,
        )


def load_basins_forcing(gauge_id_list: Optional[List[str]] = None,
                        root_path: str = params.camels_root_path,
                        source: str = params.precipitation_source,
                        ignore_columns: Optional[List[str]] = None,
                        multi_process: bool = False,
                        ) -> DataFrame:
    """
    多线程加载CAMELS数据集中所有流域的降水数据

    :param gauge_id_list: 流域ID列表
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param source: 降水数据来源, 默认为"daymet", 可选值为"daymet"、“maurer"和”nldas"
    :param ignore_columns: 忽略的列名列表
    :param multi_process: 是否使用多进程
    :return: 所有流域的降水数据
    """
    # 获取流域ID列表和对应的HUC02编码
    if gauge_id_list is None:  # 如果没有指定流域ID列表，则加载所有流域的ID, 并获取对应的HUC02编码
        gauge_id_list, huc_02_list = get_gauge_id(root_path, True)
    else:  # 如果指定了流域ID列表，则获取对应的HUC02编码
        huc_02_list = load_single_type_attributes("name", root_path).loc[gauge_id_list, "huc_02"].tolist()
    # 判断是否使用多进程, 并加载所有流域的降水数据
    if multi_process:
        num_cores = cpu_count()
        # 先将流域ID列表和huc_02列表分成num_cores份
        gauge_id_lists = split_list(gauge_id_list, num_cores)
        huc_02_lists = split_list(huc_02_list, num_cores)
        # 使用多进程加载所有流域的降水数据
        with ProcessPoolExecutor() as executor:
            return pd.concat(executor.map(load_basins_forcing_with_threads,
                                          gauge_id_lists, huc_02_lists,
                                          [root_path] * num_cores,
                                          [source] * num_cores,
                                          [ignore_columns] * num_cores,
                                          ),
                             axis=1, )
    else:
        return load_basins_forcing_with_threads(gauge_id_list, huc_02_list,
                                                root_path, source, ignore_columns)


def load_single_basin_streamflow(gauge_id: str,
                                 huc_02: Optional[str] = None,
                                 root_path: str = params.camels_root_path,
                                 add_datetime: bool = True,
                                 ) -> DataFrame:
    """
    加载单个流域的流量数据

    :param gauge_id: 流域ID
    :param huc_02: 流域的HUC02编码，如果没有指定，则从流域名称数据中获取
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param add_datetime: 是否添加datetime列，若添加，将删去Year，Mnth和Day三列，并添加datetime列为索引
    :return: 单个流域的流量数据
    """
    # 如果没有指定HUC02编码，则从流域名称数据中获取
    if huc_02 is None:
        huc_02 = load_single_type_attributes("name", root_path).loc[gauge_id, "huc_02"]
    # 读取数据
    path = root_path + f"/basin_timeseries_v1p2_metForcing_obsFlow/basin_dataset_public_v1p2/usgs_streamflow/{huc_02}/{gauge_id}_streamflow_qc.txt"
    data = pd.read_csv(path,
                       header=None,
                       names=["gauge_id", "Year", "Mnth", "Day", "streamflow", "qc"],
                       sep="\s+",
                       )
    # 数据处理
    data["streamflow"] = data["streamflow"].where(data["qc"] != "M")  # 如果qc是M，则将流量数据设为NaN
    data.drop(columns="qc", inplace=True)
    if add_datetime:  # 根据年月日形成datetime列
        data["datetime"] = pd.to_datetime(data[["Year", "Mnth", "Day"]].astype(str).agg("-".join, axis=1))
        data.set_index("datetime", inplace=True)
        data.drop(columns=["Year", "Mnth", "Day"], inplace=True)
    data.drop(columns="gauge_id", inplace=True)  # 删除gauge_id列
    return data


def load_basins_streamflow_with_threads(gauge_id_list: List[str],
                                        huc_02_list: List[str],
                                        root_path: str,
                                        ) -> DataFrame:
    """
    多线程加载CAMELS数据集中部分流域的流量数据

    :param gauge_id_list: 流域ID列表
    :param huc_02_list: 流域的HUC02编码列表
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :return: 所有流域的流量数据
    """
    with ThreadPoolExecutor() as executor:  # 使用多线程加载所有流域的流量数据
        return pd.concat(executor.map(load_single_basin_streamflow,
                                      gauge_id_list, huc_02_list,
                                      [root_path] * len(gauge_id_list),
                                      ),
                         keys=gauge_id_list,
                         names=["gauge_id", "timeseries_type"],
                         axis=1, )


def load_basins_streamflow(gauge_id_list: Optional[List[str]] = None,
                           root_path: str = params.camels_root_path,
                           multi_process: bool = False,
                           ) -> DataFrame:
    """
    加载CAMELS数据集中所有流域的流量数据

    :param gauge_id_list: 流域ID列表
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param multi_process: 是否使用多进程
    :return: 所有流域的流量数据
    """
    # 如果没有指定流域ID列表，则加载所有流域的ID
    if gauge_id_list is None:
        gauge_id_list, huc_02_list = get_gauge_id(root_path, True)
    else:
        huc_02_list = load_single_type_attributes("name", root_path).loc[gauge_id_list, "huc_02"].tolist()
    # 判断使用多线程还是多进程, 并加载所有流域的流量数据
    if multi_process:
        num_cores = cpu_count()
        # 先将流域ID列表和huc_02列表分成num_cores份
        gauge_id_lists = split_list(gauge_id_list, num_cores)
        huc_02_lists = split_list(huc_02_list, num_cores)
        # 使用多进程加载所有流域的流量数据
        with ProcessPoolExecutor() as executor:
            return pd.concat(
                executor.map(load_basins_streamflow_with_threads,
                             gauge_id_lists, huc_02_lists,
                             [root_path] * num_cores,
                             ),
                axis=1,
            )
    else:
        return load_basins_streamflow_with_threads(gauge_id_list, huc_02_list, root_path)


def load_single_basin_timeseries(gauge_id: str,
                                 huc_02: Optional[str] = None,
                                 root_path: str = params.camels_root_path,
                                 source: str = params.precipitation_source,
                                 ignore_columns: Optional[List[str]] = None,
                                 add_datetime: bool = True,
                                 ) -> DataFrame:
    """
    加载单个流域的降水和流量数据

    :param gauge_id: 流域ID
    :param huc_02: 流域的HUC02编码
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param source: 降水数据来源, 默认为"daymet", 可选值为"daymet"、“maurer"和”nldas"
    :param ignore_columns: 忽略的列名列表
    :return: 单个流域的降水和流量数据
    """
    forcing = load_single_basin_forcing(gauge_id, huc_02, root_path, source, ignore_columns)
    streamflow = load_single_basin_streamflow(gauge_id, huc_02, root_path)
    timeseries = pd.concat([forcing, streamflow], axis=1)
    return timeseries


def load_basins_timeseries_with_threads(gauge_id_list: List[str],
                                        huc_02_list: List[str],
                                        root_path: str,
                                        source: str,
                                        ignore_columns: Optional[List[str]],
                                        ) -> DataFrame:
    """
    多线程加载CAMELS数据集中部分流域的降水和流量数据

    :param gauge_id_list: 流域ID列表
    :param huc_02_list: 流域的HUC02编码列表
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param source: 降水数据来源, 默认为"daymet", 可选值为"daymet"、“maurer"和”nldas"
    :param ignore_columns: 忽略的列名列表
    :return: 所有流域的降水和流量数据
    """
    with ThreadPoolExecutor(4) as executor:  # 使用多线程加载所有流域的降水和流量数据
        return pd.concat(executor.map(load_single_basin_timeseries,
                                      gauge_id_list, huc_02_list,
                                      [root_path] * len(gauge_id_list),
                                      [source] * len(gauge_id_list),
                                      [ignore_columns] * len(gauge_id_list),
                                      ),
                         keys=gauge_id_list,
                         names=["gauge_id", "timeseries_type"],
                         axis=1, )


def load_basins_timeseries(gauge_id_list: Optional[List[str]] = None,
                           root_path: str = params.camels_root_path,
                           source: str = params.precipitation_source,
                           ignore_columns: Optional[List[str]] = None,
                           multi_process: bool = False,
                           ) -> DataFrame:
    """
    加载CAMELS数据集中所有流域的降水和流量数据

    :param gauge_id_list: 流域ID列表
    :param root_path: CAMELS数据集根目录，默认为"data/camels"
    :param source: 降水数据来源, 默认为"daymet", 可选值为"daymet"、“maurer"和”nldas"
    :param ignore_columns: 忽略的列名列表
    :param multi_process: 是否使用多进程
    :return: 所有流域的降水和流量数据
    """
    # 如果没有指定流域ID列表，则加载所有流域的ID
    if gauge_id_list is None:
        gauge_id_list, huc_02_list = get_gauge_id(root_path, True)
    else:
        huc_02_list = load_single_type_attributes("name", root_path).loc[gauge_id_list, "huc_02"].tolist()
    # 判断使用多线程还是多进程, 并加载所有流域的降水和流量数据
    if multi_process:
        num_cores = cpu_count()
        # 先将流域ID列表和huc_02列表分成num_cores份
        gauge_id_lists = split_list(gauge_id_list, num_cores)
        huc_02_lists = split_list(huc_02_list, num_cores)
        # 使用多进程加载所有流域的降水和流量数据
        with ProcessPoolExecutor() as executor:
            return pd.concat(
                executor.map(load_basins_timeseries_with_threads,
                             gauge_id_lists, huc_02_lists,
                             [root_path] * num_cores,
                             [source] * num_cores,
                             [ignore_columns] * num_cores,
                             ),
                axis=1,
            )
    else:
        return load_basins_timeseries_with_threads(gauge_id_list, huc_02_list, root_path, source, ignore_columns)
```