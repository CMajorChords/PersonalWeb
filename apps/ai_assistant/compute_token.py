import tiktoken
import streamlit as st
from tiktoken import Encoding
from math import ceil


def count_str_message_tokens(message: dict,
                             model_encoding: Encoding,
                             base_tokens: int = 6,
                             ) -> int:
    """
    è®¡ç®—å•ä¸ªæ–‡æœ¬æ¶ˆæ¯çš„tokenæ•°é‡

    :param message: å•ä¸ªæ–‡æœ¬æ¶ˆæ¯
    :param model_encoding: æ¨¡å‹ç¼–ç 
    :param base_tokens: æ¯ä¸ªmessageçš„é¢å¤–tokenæ•°é‡
    :return: å•ä¸ªæ–‡æœ¬æ¶ˆæ¯çš„tokenæ•°é‡
    """
    message_tokens = model_encoding.encode(message["content"])
    return len(message_tokens) + base_tokens


def count_image_message_tokens(message: dict,
                               base_tokens: int,
                               tile_tokens: int,
                               tile_size: int,
                               max_tokens: int,
                               ) -> int:
    """
    è®¡ç®—å•ä¸ªå›¾ç‰‡æ¶ˆæ¯çš„tokenæ•°é‡ï¼Œå›¾ç‰‡æ ¼å¼åº”è¯¥ä¸ºPIL.Image

    :param message: å•ä¸ªå›¾ç‰‡æ¶ˆæ¯
    :param base_tokens: æ¯ä¸ªmessageçš„é¢å¤–tokenæ•°é‡
    :param tile_tokens: æ¯ä¸ªtileçš„tokenæ•°é‡
    :param tile_size: tileçš„å¤§å°
    :param max_tokens: å•ä¸ªå›¾ç‰‡æ¶ˆæ¯çš„æœ€å¤§tokenæ•°é‡
    :return: å•ä¸ªå›¾ç‰‡æ¶ˆæ¯çš„tokenæ•°é‡
    """
    width, height = message["content"].size
    num_tiles = ceil(width / tile_size) * ceil(height / tile_size)
    image_tokens = num_tiles * tile_tokens
    if image_tokens > max_tokens:
        image_tokens = max_tokens
    return base_tokens + image_tokens


def count_messages_tokens(messages_input: list,
                          messages_show: list,
                          message_prompt_template: list,
                          model: str,
                          base_tokens_str_message: int = 3,
                          base_tokens_image_message: int = 85 + 3,
                          tile_tokens: int = 170,
                          tile_size: int = 512,
                          image_max_tokens: int = 1445,
                          ) -> (int, int):
    """
    è®¡ç®—æ¶ˆæ¯çš„tokenæ•°é‡.
    ä»message_inputä¸­åˆ¤æ–­æ¶ˆæ¯ç±»å‹ã€‚
    å¦‚æœæ˜¯æ–‡æœ¬æ¶ˆæ¯ï¼Œå°†ä»messages_inputä¸­è·å–ã€‚
    å¦‚æœæ˜¯å›¾ç‰‡æ¶ˆæ¯ï¼Œå°†ä»messages_showä¸­è·å–ã€‚

    :param messages_input: è¾“å…¥è¿›clientçš„æ¶ˆæ¯
    :param messages_show: æ˜¾ç¤ºåœ¨UIä¸Šçš„æ¶ˆæ¯
    :param message_prompt_template: æç¤ºè¯æ¨¡æ¿
    :param model: æ¨¡å‹åç§°
    :param base_tokens_str_message: æ¯ä¸ªæ–‡æœ¬messageçš„é¢å¤–tokenæ•°é‡
    :param base_tokens_image_message: æ¯ä¸ªå›¾ç‰‡messageçš„é¢å¤–tokenæ•°é‡
    :param tile_tokens: è®¡ç®—å›¾ç‰‡tokensæ—¶æ¯ä¸ªtileçš„tokenæ•°é‡
    :param tile_size: è®¡ç®—å›¾ç‰‡tokensæ—¶tileçš„å¤§å°
    :param image_max_tokens: å•ä¸ªå›¾ç‰‡æ¶ˆæ¯çš„æœ€å¤§tokenæ•°é‡
    :return: æ¶ˆæ¯çš„tokenæ•°é‡
    """
    # é…ç½®æ¨¡å‹ç¼–ç 
    if model == "gpt 4" or model == "gpt 4o":
        model += "-"
    model_encoding = tiktoken.encoding_for_model(model)
    # è®¡ç®—input tokens
    messages_input_prompt = messages_input[:-1]
    messages_show_prompt = messages_show[:-1]
    prompt_str_tokens, prompt_image_tokens = 0, 0  # 3 tokens for start token, end token and separator token
    for idx, message in enumerate(messages_input_prompt):
        if isinstance(message["content"], str):
            prompt_str_tokens += count_str_message_tokens(message=message,
                                                          model_encoding=model_encoding,
                                                          base_tokens=base_tokens_str_message)
        else:
            message = messages_show_prompt[idx]
            prompt_image_tokens += count_image_message_tokens(message=message,
                                                              base_tokens=base_tokens_image_message,
                                                              tile_tokens=tile_tokens,
                                                              tile_size=tile_size,
                                                              max_tokens=image_max_tokens,
                                                              )
    # è®¡ç®—output tokens
    message_input_output = messages_input[-1]
    message_show_output = messages_show[-1]
    if isinstance(message_input_output["content"], str):
        output_tokens = count_str_message_tokens(message=message_input_output,
                                                 model_encoding=model_encoding,
                                                 base_tokens=base_tokens_str_message)
    else:
        output_tokens = count_image_message_tokens(message=message_show_output,
                                                   base_tokens=base_tokens_image_message,
                                                   tile_tokens=tile_tokens,
                                                   tile_size=tile_size,
                                                   max_tokens=image_max_tokens,
                                                   )
    output_tokens += 3
    # è®¡ç®—template tokens
    prompt_str_tokens += count_str_message_tokens(message=message_prompt_template[0],
                                                  model_encoding=model_encoding,
                                                  base_tokens=base_tokens_str_message)
    return prompt_str_tokens, prompt_image_tokens, output_tokens


def compute_token_price(messages_input: list,
                        messages_show: list,
                        message_prompt_template: list,
                        model: str,
                        ):
    """
    è®¡ç®—æœ¬æ¬¡æé—®èŠ±è´¹

    :param messages_input: è¾“å…¥è¿›clientçš„æ¶ˆæ¯
    :param messages_show: æ˜¾ç¤ºåœ¨UIä¸Šçš„æ¶ˆæ¯
    :param message_prompt_template: æç¤ºè¯æ¨¡æ¿
    :param model: æ¨¡å‹åç§°
    """
    # åˆ¤æ–­messageæ˜¯å¦ä¸ºç©º
    if st.session_state["language"] == "ä¸­æ–‡":
        label = "è®¡ç®—æœ¬æ¬¡æé—®èŠ±è´¹"
        str_prompt_token = "æœ¬æ¬¡æé—®è¾“å…¥tokenæ•°é‡ï¼š"
        str_output_token = "æœ¬æ¬¡æé—®è¾“å‡ºtokenæ•°é‡ï¼š"
        str_token_price = "æœ¬æ¬¡æé—®èŠ±è´¹ï¼š"
    else:
        label = "Cost of this question"
        str_prompt_token = "Input tokens:"
        str_output_token = "Output tokens:"
        str_token_price = "Cost:"
    if st.button(label=label):
        if messages_input:
            price_table = {
                "gpt-4o-ca": (0.02, 0.0289, 0.06),
                "gpt-4-turbo-ca": (0.04, 0.0578, 0.12),
                "gpt-4-ca": (0.12, 0.12, 0.24),
                "gpt-4o-mini": (0.00105, 0.035, 0.0042),
                "gpt-3.5-turbo-ca": (0.001, 0.001, 0.003),
            }
            prompt_str_tokens, prompt_image_tokens, output_tokens = count_messages_tokens(
                messages_input=messages_input,
                messages_show=messages_show,
                message_prompt_template=message_prompt_template,
                model=model)
            price = price_table[model]
            token_price = (prompt_str_tokens * price[0] +
                           prompt_image_tokens * price[1] +
                           output_tokens * price[2]) / 1000
            # token priceåº”è¯¥ä¿ç•™ä¸‰ä½å°æ•°
            st.toast(f"{str_prompt_token}{prompt_str_tokens + prompt_image_tokens}", icon="ğŸª™")
            st.toast(f"{str_output_token}{output_tokens}", icon="ğŸª™")
            st.toast(f"**{str_token_price}{token_price:.3f}ï¿¥**", icon="ğŸª™")
        else:
            st.toast(f"{str_prompt_token}0", icon="ğŸª™")
            st.toast(f"{str_output_token} 0", icon="ğŸª™")
            st.toast(f"**{str_token_price} 0ï¿¥**", icon="ğŸª™")

    # gpt-3.5-turbo-ca	0.001 / 1K Tokens	0.003 / 1K Tokens	æ”¯æŒ	Azure openaiä¸­è½¬(ä¹Ÿå±äºå®˜æ–¹æ¨¡å‹çš„ä¸€ç§)ä»·æ ¼ä¾¿å®œ, ä½†æ˜¯å›å¤çš„æ…¢ä¸€äº›
    # gpt-3.5-turbo	0.0035 / 1K Tokens	0.0105 / 1K Tokens	æ”¯æŒ	é»˜è®¤æ¨¡å‹ï¼Œç­‰äºgpt-3.5-turbo-0125
    # gpt-3.5-turbo-1106	0.007 / 1K Tokens	0.014 / 1K Tokens	æ”¯æŒ	2023å¹´11æœˆ6æ—¥æ›´æ–°çš„æ¨¡å‹
    # gpt-3.5-turbo-0125	0.0035 / 1K Tokens	0.0105 / 1K Tokens	æ”¯æŒ	2024å¹´1æœˆ25æ—¥æœ€æ–°æ¨¡å‹ï¼Œæ•°æ®æœ€æ–°ï¼Œä»·æ ¼æ›´æ›´ä½ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œä¿®å¤äº†ä¸€äº›1106çš„bugã€‚
    # gpt-3.5-turbo-0301	0.0105 / 1K Tokens	0.014 / 1K Tokens	æ”¯æŒ	é€‚åˆå¿«é€Ÿå›ç­”ç®€å•é—®é¢˜
    # gpt-3.5-turbo-0613	0.0105 / 1K Tokens	0.014 / 1K Tokens	æ”¯æŒ	é€‚åˆå¿«é€Ÿå›ç­”ç®€å•é—®é¢˜ï¼Œæ”¯æŒFunction
    # gpt-3.5-turbo-16k	0.021 / 1K Tokens	0.028 / 1K Tokens	æ”¯æŒ	é€‚åˆå¿«é€Ÿå›ç­”ç®€å•é—®é¢˜,å­—æ•°æ›´å¤š
    # gpt-3.5-turbo-16k-0613	0.021 / 1K Tokens	0.028 / 1K Tokens	æ”¯æŒ	é€‚åˆå¿«é€Ÿå›ç­”ç®€å•é—®é¢˜ï¼Œå­—æ•°æ›´å¤šï¼Œæ”¯æŒFunction
    # gpt-4	0.21 / 1K Tokens	0.42 / 1K Tokens	æ”¯æŒ	é»˜è®¤æ¨¡å‹ï¼Œç­‰äºgpt-4-0613
    # gpt-4o	0.035/1K Tokens + å›¾ç‰‡è´¹ç”¨[2]	0.105/1K Tokens / 1K Tokens	æ”¯æŒ	Openai æœ€æ–°æ¨¡å‹, ä»·æ ¼æ›´ä½, é€Ÿåº¦æ›´å¿«æ›´èªæ˜
    # gpt-4o-mini	0.00105/1K Tokens + å›¾ç‰‡è´¹ç”¨[2]	0.0042/1K Tokens / 1K Tokens	æ”¯æŒ	Openai æœ€æ–°æ¨¡å‹, ä»·æ ¼æ›´ä½, è¾“å‡ºè´¨é‡åœ¨3.5ä¹‹ä¸Š4oä¹‹ä¸‹, å¹¶ä¸”æ”¯æŒè¯»å›¾
    # gpt-4-0613	0.21 / 1K Tokens	0.42 / 1K Tokens	æ”¯æŒ	2023å¹´6æœˆ13æ—¥æ›´æ–°çš„æ¨¡å‹
    # gpt-4-turbo-preview	0.07 / 1K Tokens	0.21 / 1K Tokens	æ”¯æŒ	æœ€æ–°æ¨¡å‹ï¼Œè¾“å…¥128Kï¼Œè¾“å‡ºæœ€å¤§4Kï¼ŒçŸ¥è¯†åº“æœ€æ–°2023å¹´4æœˆ, æ­¤æ¨¡å‹å§‹ç»ˆæŒ‡å‘æœ€æ–°çš„4çš„previewæ¨¡å‹
    # gpt-4-0125-preview	0.07 / 1K Tokens	0.21 / 1K Tokens	æ”¯æŒ	2024å¹´1æœˆ25æ—¥æ›´æ–°çš„æ¨¡å‹ï¼Œè¾“å…¥128Kï¼Œè¾“å‡ºæœ€å¤§4Kï¼ŒçŸ¥è¯†åº“æœ€æ–°2023å¹´4æœˆ, ä¿®å¤äº†ä¸€äº›1106çš„bug
    # gpt-4-1106-preview	0.07 / 1K Tokens	0.21 / 1K Tokens	æ”¯æŒ	2023å¹´11æœˆ6æ—¥æ›´æ–°çš„æ¨¡å‹ï¼Œè¾“å…¥128Kï¼Œè¾“å‡ºæœ€å¤§4Kï¼ŒçŸ¥è¯†åº“æœ€æ–°2023å¹´4æœˆ
    # gpt-4-vision-preview	0.07 / 1K Tokens + å›¾ç‰‡è´¹ç”¨[2]	0.21 / 1K Tokens	æ”¯æŒ	å¤šæ¨¡æ€ï¼Œæ”¯æŒå›¾ç‰‡è¯†åˆ«
    # gpt-4-turbo	0.07 / 1K Tokens + å›¾ç‰‡è´¹ç”¨[2]	0.21 / 1K Tokens	æ”¯æŒ	Openai æœ€æ–°æ¨¡å‹å¤šæ¨¡æ€ï¼Œæ”¯æŒå›¾ç‰‡è¯†åˆ«ï¼Œæ”¯æŒå‡½æ•°tools
    # gpt-4-turbo-2024-04-09	0.07 / 1K Tokens + 0.10115*å›¾ç‰‡ä¸ªæ•°[3]	0.21 / 1K Tokens	æ”¯æŒ	Openai æœ€æ–°æ¨¡å‹å¤šæ¨¡æ€ï¼Œæ”¯æŒå›¾ç‰‡è¯†åˆ«ï¼Œæ”¯æŒå‡½æ•°tools
    # gpt-4-ca	0.12 / 1K Tokens	0.24 / 1K Tokens	æ”¯æŒ	Azure openaiä¸­è½¬ å¯¹æ ‡gpt-4(ä¹Ÿå±äºå®˜æ–¹æ¨¡å‹çš„ä¸€ç§)ä»·æ ¼ä¾¿å®œ
    # gpt-4-turbo-ca	0.04 / 1K Tokens + 0.0578*å›¾ç‰‡ä¸ªæ•°[3]	0.12 / 1K Tokens	æ”¯æŒ	Azure openaiä¸­è½¬ å¯¹æ ‡gpt-4-turbo(ä¹Ÿå±äºå®˜æ–¹æ¨¡å‹çš„ä¸€ç§)ä»·æ ¼ä¾¿å®œ, ä½†æ˜¯å›å¤çš„æ…¢ä¸€äº›
    # gpt-4o-ca	0.02 / 1K Tokens + 0.0289*å›¾ç‰‡ä¸ªæ•°[2]	0.06 / 1K Tokens	æ”¯æŒ	Azure openaiä¸­è½¬ å¯¹æ ‡gpt-4o(ä¹Ÿå±äºå®˜æ–¹æ¨¡å‹çš„ä¸€ç§)ä»·æ ¼ä¾¿å®œ
    # gpt-3.5-turbo-instruct	0.0105 / 1K Tokens	0.014 / 1K Tokens	æ”¯æŒ	Completionsæ¨¡å‹ ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼Œæä¾›å‡†ç¡®çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ä¸€èˆ¬äººç”¨ä¸ä¸Š
